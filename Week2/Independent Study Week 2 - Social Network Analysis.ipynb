{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1 - Hand Calculated Graph Centrality\n",
    "\n",
    "In this part, we will load the Kite network data and perform a graph centrality algorithm by hand. First we load the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_vertices.csv\n",
      "PROGRESS: Parsing completed. Parsed 10 lines in 0.018072 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_vertices.csv\n",
      "PROGRESS: Parsing completed. Parsed 10 lines in 0.017847 secs.\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_edges.csv\n",
      "PROGRESS: Parsing completed. Parsed 18 lines in 0.019082 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_edges.csv\n",
      "PROGRESS: Parsing completed. Parsed 18 lines in 0.016303 secs.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(\"head\").append($(\"<link/>\").attr({\n",
       "  rel:  \"stylesheet\",\n",
       "  type: \"text/css\",\n",
       "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
       "}));\n",
       "$(\"head\").append($(\"<link/>\").attr({\n",
       "  rel:  \"stylesheet\",\n",
       "  type: \"text/css\",\n",
       "  href: \"//dato.com/files/canvas/1.4.1/css/canvas.css\"\n",
       "}));\n",
       "\n",
       "            (function(){\n",
       "\n",
       "                var e = null;\n",
       "                if (typeof element == 'undefined') {\n",
       "                    var scripts = document.getElementsByTagName('script');\n",
       "                    var thisScriptTag = scripts[scripts.length-1];\n",
       "                    var parentDiv = thisScriptTag.parentNode;\n",
       "                    e = document.createElement('div');\n",
       "                    parentDiv.appendChild(e);\n",
       "                } else {\n",
       "                    e = element[0];\n",
       "                }\n",
       "\n",
       "                require(['//dato.com/files/canvas/1.4.1/js/ipython_app.js'], function(IPythonApp){\n",
       "                    var app = new IPythonApp();\n",
       "                    app.attachView('sgraph','View', {\"edges_labels\": null, \"selected_variable\": {\"name\": [\"g_kite\"], \"view_component\": \"View\", \"view_file\": \"sgraph\", \"view_params\": {\"vlabel_hover\": false, \"arrows\": false, \"ewidth\": 1, \"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 300, \"elabel\": null, \"ecolor\": [0.37, 0.33, 0.33], \"vlabel\": \"id\", \"vcolor\": [0.522, 0.741, 0.0], \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 18, \"vertices\": 10}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Beverly\", \"Fernando\", \"Jane\", \"Ed\", \"Diane\", \"Garth\", \"Andre\", \"Carol\", \"Ike\", \"Heather\"], \"vertices_labels\": [\"Beverly\", \"Fernando\", \"Jane\", \"Ed\", \"Diane\", \"Garth\", \"Andre\", \"Carol\", \"Ike\", \"Heather\"], \"edges\": [[\"Beverly\", \"Diane\"], [\"Beverly\", \"Ed\"], [\"Beverly\", \"Garth\"], [\"Fernando\", \"Garth\"], [\"Fernando\", \"Heather\"], [\"Diane\", \"Fernando\"], [\"Diane\", \"Ed\"], [\"Diane\", \"Garth\"], [\"Ed\", \"Garth\"], [\"Garth\", \"Heather\"], [\"Andre\", \"Beverly\"], [\"Andre\", \"Fernando\"], [\"Andre\", \"Diane\"], [\"Andre\", \"Carol\"], [\"Carol\", \"Fernando\"], [\"Carol\", \"Diane\"], [\"Ike\", \"Jane\"], [\"Heather\", \"Ike\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
       "                });\n",
       "            })();\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hide some silly output\n",
    "import logging\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# Import everything we need\n",
    "import graphlab as gl\n",
    "\n",
    "# Load Data\n",
    "kite_vertices = gl.SFrame.read_csv('../Week1/kite_vertices.csv')\n",
    "kite_edges = gl.SFrame.read_csv('../Week1/kite_edges.csv')\n",
    "\n",
    "# Create graph\n",
    "g_kite = gl.SGraph()\n",
    "g_kite = g_kite.add_vertices(vertices=kite_vertices, vid_field='name')\n",
    "g_kite = g_kite.add_edges(edges=kite_edges, src_field='src', dst_field='dst')\n",
    "\n",
    "# Visualize graph?\n",
    "gl.canvas.set_target('ipynb')\n",
    "g_kite.show(vlabel=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Crawling Social Data\n",
    "\n",
    "In this part, I will use the Twitter / Facebook / LinkedIn API to download a graphical data of my portion of the social graph, and attempt to visualize it in Gephi or Neo4J.\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Download facebook python api\n",
    "- Setup API keys for personal account\n",
    "- Query FB for my friends (and maybe friends of friends) and their data\n",
    "- Display as a graph somehow\n",
    "  - Show ages of friends in different colors?\n",
    "  - With friends of friends, can I detect clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Textual Analysis of Tweets from Political ThinkTanks\n",
    "\n",
    "In this part, I will download the tweet streams from different political 'think tanks' and perform a simple frequency analysis to see if there are any insights we can derive about the political leanings of these institutions. As an example of what tweet strams I will parse:\n",
    "\n",
    "- https://twitter.com/fairmediawatch - Fairness and Accuracy In Reporting\n",
    "- https://twitter.com/AccuracyInMedia - Accuracy In Media\n",
    "- https://twitter.com/ips_dc - Institute for Policy Studies\n",
    "- https://twitter.com/heritage - Heritage Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
