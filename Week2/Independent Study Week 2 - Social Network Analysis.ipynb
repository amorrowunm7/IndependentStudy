{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1 - Graph Centrality Measures\n",
    "\n",
    "In this part, we will load the Kite network data and perform a graph centrality algorithm by hand. First we load the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_vertices.csv\n",
      "PROGRESS: Parsing completed. Parsed 10 lines in 0.034121 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_vertices.csv\n",
      "PROGRESS: Parsing completed. Parsed 10 lines in 0.019032 secs.\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_edges.csv\n",
      "PROGRESS: Parsing completed. Parsed 18 lines in 0.019352 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week1/kite_edges.csv\n",
      "PROGRESS: Parsing completed. Parsed 18 lines in 0.016459 secs.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(\"head\").append($(\"<link/>\").attr({\n",
       "  rel:  \"stylesheet\",\n",
       "  type: \"text/css\",\n",
       "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
       "}));\n",
       "$(\"head\").append($(\"<link/>\").attr({\n",
       "  rel:  \"stylesheet\",\n",
       "  type: \"text/css\",\n",
       "  href: \"//dato.com/files/canvas/1.4.1/css/canvas.css\"\n",
       "}));\n",
       "\n",
       "            (function(){\n",
       "\n",
       "                var e = null;\n",
       "                if (typeof element == 'undefined') {\n",
       "                    var scripts = document.getElementsByTagName('script');\n",
       "                    var thisScriptTag = scripts[scripts.length-1];\n",
       "                    var parentDiv = thisScriptTag.parentNode;\n",
       "                    e = document.createElement('div');\n",
       "                    parentDiv.appendChild(e);\n",
       "                } else {\n",
       "                    e = element[0];\n",
       "                }\n",
       "\n",
       "                require(['//dato.com/files/canvas/1.4.1/js/ipython_app.js'], function(IPythonApp){\n",
       "                    var app = new IPythonApp();\n",
       "                    app.attachView('sgraph','View', {\"edges_labels\": null, \"selected_variable\": {\"name\": [\"g_kite\"], \"view_component\": \"View\", \"view_file\": \"sgraph\", \"view_params\": {\"vlabel_hover\": false, \"arrows\": false, \"ewidth\": 1, \"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 300, \"elabel\": null, \"ecolor\": [0.37, 0.33, 0.33], \"vlabel\": \"id\", \"vcolor\": [0.522, 0.741, 0.0], \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 36, \"vertices\": 10}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Beverly\", \"Fernando\", \"Diane\", \"Jane\", \"Ed\", \"Garth\", \"Andre\", \"Carol\", \"Ike\", \"Heather\"], \"vertices_labels\": [\"Beverly\", \"Fernando\", \"Diane\", \"Jane\", \"Ed\", \"Garth\", \"Andre\", \"Carol\", \"Ike\", \"Heather\"], \"edges\": [[\"Beverly\", \"Diane\"], [\"Beverly\", \"Ed\"], [\"Fernando\", \"Diane\"], [\"Beverly\", \"Garth\"], [\"Fernando\", \"Garth\"], [\"Beverly\", \"Andre\"], [\"Fernando\", \"Andre\"], [\"Fernando\", \"Carol\"], [\"Fernando\", \"Heather\"], [\"Diane\", \"Fernando\"], [\"Diane\", \"Beverly\"], [\"Ed\", \"Beverly\"], [\"Diane\", \"Ed\"], [\"Ed\", \"Diane\"], [\"Ed\", \"Garth\"], [\"Diane\", \"Garth\"], [\"Diane\", \"Andre\"], [\"Diane\", \"Carol\"], [\"Jane\", \"Ike\"], [\"Garth\", \"Beverly\"], [\"Garth\", \"Fernando\"], [\"Garth\", \"Diane\"], [\"Garth\", \"Ed\"], [\"Garth\", \"Heather\"], [\"Andre\", \"Beverly\"], [\"Andre\", \"Fernando\"], [\"Andre\", \"Diane\"], [\"Andre\", \"Carol\"], [\"Carol\", \"Fernando\"], [\"Carol\", \"Diane\"], [\"Carol\", \"Andre\"], [\"Heather\", \"Fernando\"], [\"Ike\", \"Jane\"], [\"Heather\", \"Garth\"], [\"Heather\", \"Ike\"], [\"Ike\", \"Heather\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
       "                });\n",
       "            })();\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hide some silly output\n",
    "import logging\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# Import everything we need\n",
    "import graphlab as gl\n",
    "\n",
    "# Load Data\n",
    "kite_vertices = gl.SFrame.read_csv('../Week1/kite_vertices.csv')\n",
    "kite_edges = gl.SFrame.read_csv('../Week1/kite_edges.csv')\n",
    "\n",
    "# Create graph\n",
    "g_kite = gl.SGraph()\n",
    "g_kite = g_kite.add_vertices(vertices=kite_vertices, vid_field='name')\n",
    "g_kite = g_kite.add_edges(edges=kite_edges, src_field='src', dst_field='dst')\n",
    "g_kite = g_kite.add_edges(edges=kite_edges, src_field='dst', dst_field='src')\n",
    "\n",
    "# Visualize graph?\n",
    "gl.canvas.set_target('ipynb')\n",
    "g_kite.show(vlabel=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will look at degree centrality, closeness centrality, betweenness centrality, eigenvector centrality and PageRank.\n",
    "\n",
    "## Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">__id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">out_degree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Beverly</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Fernando</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Diane</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Jane</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Ed</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Garth</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Andre</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Carol</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Ike</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Heather</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 2 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\t__id\tstr\n",
       "\tout_degree\tint\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+----------+------------+\n",
       "|   __id   | out_degree |\n",
       "+----------+------------+\n",
       "| Beverly  |     4      |\n",
       "| Fernando |     5      |\n",
       "|  Diane   |     6      |\n",
       "|   Jane   |     1      |\n",
       "|    Ed    |     3      |\n",
       "|  Garth   |     5      |\n",
       "|  Andre   |     4      |\n",
       "|  Carol   |     3      |\n",
       "|   Ike    |     2      |\n",
       "| Heather  |     3      |\n",
       "+----------+------------+\n",
       "[10 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphlab import degree_counting\n",
    "deg = degree_counting.create(g_kite)\n",
    "deg_graph = deg['graph']\n",
    "in_degree = deg_graph.vertices[['__id', 'in_degree']]\n",
    "in_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Diane node would be considered most central by this metric, which is true: Diane has the most connections\n",
    "\n",
    "## Closeness Centrality\n",
    "\n",
    "Closeness centrality is computed by finding the shortest paths from all nodes to another, and then for each node, computing an average distance to all other nodes, dividing that by the maximum distance, and taking a reciprocial. Graphlab allows us to find shortest paths starting from a node. We will use that and create an algorithm to calculate this measure for all nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Andre': 2.3529411764705883,\n",
       " 'Beverly': 2.3529411764705883,\n",
       " 'Carol': 2.2222222222222223,\n",
       " 'Diane': 2.6666666666666665,\n",
       " 'Ed': 2.2222222222222223,\n",
       " 'Fernando': 2.857142857142857,\n",
       " 'Garth': 2.857142857142857,\n",
       " 'Heather': 2.6666666666666665,\n",
       " 'Ike': 1.9047619047619047,\n",
       " 'Jane': 1.3793103448275863}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphlab import shortest_path\n",
    "\n",
    "# Foind shortest paths to all nodes for each vertex in graph\n",
    "shortestPaths = {}\n",
    "for vertex in g_kite.get_vertices():\n",
    "    shortestPaths[vertex['__id']] = shortest_path.create(g_kite, source_vid=vertex['__id'], verbose=False)\n",
    "\n",
    "# Find maximum distance\n",
    "maxDistance = 0\n",
    "for node, sp in shortestPaths.iteritems():\n",
    "    maxDistance = max(maxDistance, max(sp['distance']['distance']))\n",
    "\n",
    "# Calculate a closeness metric for all nodes\n",
    "closeness = {}\n",
    "for vertex, sp in shortestPaths.iteritems():\n",
    "    closeness[vertex] = sum(sp['distance']['distance']) / float(len(sp['distance']['distance']))\n",
    "    closeness[vertex] = maxDistance / closeness[vertex]\n",
    "closeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like an intersting measure, as we have Fernando and Garth tied for first with Diane and Heather up next. This seems more interesting than degree centrality, since some nodes may have only a few connections but may end up being on many shortest paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality\n",
    "\n",
    "Not sure how accurate this is, esp since GraphLab only returns one shortest path even if there are multiples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carol', 0.0), ('Diane', 1.0), ('Garth', 2.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = shortestPaths['Carol']\n",
    "x.get_path('Garth')\n",
    "#'Andre' in [ p[0] for p in x.get_path('Diane')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Andre': 0.1111111111111111,\n",
       " 'Beverly': 0.0,\n",
       " 'Carol': 1.0,\n",
       " 'Diane': 0.2222222222222222,\n",
       " 'Ed': 0.0,\n",
       " 'Fernando': 0.3333333333333333,\n",
       " 'Garth': 0.0,\n",
       " 'Heather': 0.2222222222222222,\n",
       " 'Ike': 0.1111111111111111,\n",
       " 'Jane': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweenness = { }\n",
    "for v in g_kite.get_vertices():\n",
    "    numShortestPathsWithVertex = 0\n",
    "    numShortestPaths = 0\n",
    "    for s in g_kite.get_vertices():\n",
    "        for t in g_kite.get_vertices():\n",
    "            if v != s and v != t and s != t:\n",
    "                sp = shortestPaths[s['__id']]\n",
    "                if v['__id'] in [ p[0] for p in x.get_path(t['__id'])]:\n",
    "                    numShortestPathsWithVertex = numShortestPathsWithVertex + 1\n",
    "                numShortestPaths = numShortestPaths + 1\n",
    "    betweenness[v['__id']] = numShortestPathsWithVertex / float(numShortestPaths)\n",
    "betweenness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Counting out degree\n",
      "PROGRESS: Done counting out degree\n",
      "PROGRESS: +-----------+-----------------------+\n",
      "PROGRESS: | Iteration | L1 change in pagerank |\n",
      "PROGRESS: +-----------+-----------------------+\n",
      "PROGRESS: | 1         | 3.32917               |\n",
      "PROGRESS: | 2         | 3.19104               |\n",
      "PROGRESS: | 3         | 2.71239               |\n",
      "PROGRESS: | 4         | 2.04452               |\n",
      "PROGRESS: | 5         | 1.2017                |\n",
      "PROGRESS: | 6         | 0.715013              |\n",
      "PROGRESS: | 7         | 0.25379               |\n",
      "PROGRESS: | 8         | 0.0283844             |\n",
      "PROGRESS: | 9         | 0                     |\n",
      "PROGRESS: +-----------+-----------------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">__id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">pagerank</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Jane</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.91209238429</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Ike</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.896579275635</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Heather</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.87832855957</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Garth</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.683254915365</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Fernando</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.347204427083</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Diane</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.310703125</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Ed</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.289563802083</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Beverly</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.181875</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Carol</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.181875</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Andre</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.15</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\t__id\tstr\n",
       "\tpagerank\tfloat\n",
       "\tdelta\tfloat\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+----------+----------------+-------+\n",
       "|   __id   |    pagerank    | delta |\n",
       "+----------+----------------+-------+\n",
       "|   Jane   | 0.91209238429  |  0.0  |\n",
       "|   Ike    | 0.896579275635 |  0.0  |\n",
       "| Heather  | 0.87832855957  |  0.0  |\n",
       "|  Garth   | 0.683254915365 |  0.0  |\n",
       "| Fernando | 0.347204427083 |  0.0  |\n",
       "|  Diane   |  0.310703125   |  0.0  |\n",
       "|    Ed    | 0.289563802083 |  0.0  |\n",
       "| Beverly  |    0.181875    |  0.0  |\n",
       "|  Carol   |    0.181875    |  0.0  |\n",
       "|  Andre   |      0.15      |  0.0  |\n",
       "+----------+----------------+-------+\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = gl.pagerank.create(g_kite)\n",
    "pr.get('pagerank').topk(column_name='pagerank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Crawling Social Data\n",
    "\n",
    "In this part, I will use the Twitter / Facebook / LinkedIn API to download a graphical data of my portion of the social graph, and attempt to visualize it in Gephi or Neo4J.\n",
    "\n",
    "## 2.1 - Facebook API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"friends\": {\n",
      "  \"data\": [], \n",
      "  \"summary\": {\n",
      "   \"total_count\": 214\n",
      "  }\n",
      " }, \n",
      " \"id\": \"684051972564\", \n",
      " \"name\": \"James Quacinella\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#ACCESS_TOKEN=\"CAAFokc3kSoEBALJY8T8qtg1q5Frfc9PYMgjBqHocZBf5a0kwfsKi0AGpZApw5iEKAZBQlVAQMCZBGcKJglbVHkZB2n2pwquMHWrZAgrhpGqHbVLbXMsmHjAvQfnHP4u1Mx2CQ0CHAJNMme9j4ozJut1MBf9V2ZCxYHZA2wVcDxZBOa9WaDwJNOAiR7wJcsZCh7Of7VvG3rvz7ZAeM4wiFvV0SRN2lfnYVwbN206oEUnJJhgogZDZD\"\n",
    "ACCESS_TOKEN=\"CAAFokc3kSoEBABwZCe10rJrxRphw9cGNd96fxJHZAY3SvhkVxXFYyGR6TcAcQgMH4wxRM0wNRNxQj0ZAD6zLE8jyWcyTTeRi0PnZApM1ykuq3U8aZBrtSiHhTLFC1X3nXcHe16vzCvAegPEvTSyHeAEtm9KBFlC4J0hbOD1nNMDWuBm8TVIxxnAKzSl0eanXLC7ZC9kyWtllmkYqZAF7Q1lkeZCi0ZCZBWZCn0ZD\"\n",
    "base_url = 'https://graph.facebook.com/me'\n",
    "\n",
    "# Get 10 likes for 10 friends\n",
    "fields = 'id,name,friends.fields(likes.limit(10))'\n",
    "url = '%s?fields=%s&access_token=%s' % (base_url, fields, ACCESS_TOKEN,)\n",
    "\n",
    "# Interpret the response as JSON and convert back to Python data structures\n",
    "content = requests.get(url).json()\n",
    "\n",
    "# Pretty-print the JSON and display it\n",
    "print json.dumps(content, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? No friends?! Researching this brought me to https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/issues/191, which states that the API only gives back results for users who give permission to the Oauth app generated by me. Much of the API calls and graph explorer are different now, so much of the book does not apply now.\n",
    "\n",
    "Moving on to LinkedIn ...\n",
    "\n",
    "## 2.2 - LinkedIn\n",
    "\n",
    "Sadly, the [same issue](https://github.com/ozgur/python-linkedin/issues/78) has arisen with LinkedIn. The API is no longer giving out access to Oauth 1.0 tokens and have substantially altered the API. Even the book's website has an [open github issue](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/issues/274) about this.\n",
    "\n",
    "## 2.3 - Twitter\n",
    "\n",
    "As a substitute, I will try to download my list of followers, and see how far I can expand this. There is a big difference here with twitter: relationships are directed, since the follow relationship is not bi-directional. Another issue to look out for are API limits. Lets start working on querying twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"status\": {\n",
      "  \"lang\": \"en\", \n",
      "  \"favorited\": false, \n",
      "  \"truncated\": false, \n",
      "  \"text\": \"\\\"Syriza\\u2019s Red Lines\\\" - http://t.co/wjKxwCI2JT\", \n",
      "  \"created_at\": \"Wed Jun 10 14:07:34 +0000 2015\", \n",
      "  \"retweeted\": false, \n",
      "  \"source\": \"<a href=\\\"http://bufferapp.com\\\" rel=\\\"nofollow\\\">Buffer</a>\", \n",
      "  \"urls\": {\n",
      "   \"http://t.co/wjKxwCI2JT\": \"http://buff.ly/1JDO0wN\"\n",
      "  }, \n",
      "  \"id\": 608636625443278848\n",
      " }, \n",
      " \"lang\": \"en\", \n",
      " \"profile_background_tile\": false, \n",
      " \"statuses_count\": 338, \n",
      " \"description\": \"Living with Analysis Paralysis\", \n",
      " \"friends_count\": 328, \n",
      " \"profile_link_color\": \"0084B4\", \n",
      " \"created_at\": \"Thu Oct 02 17:27:48 +0000 2008\", \n",
      " \"profile_sidebar_fill_color\": \"http://abs.twimg.com/images/themes/theme16/bg.gif\", \n",
      " \"utc_offset\": -14400, \n",
      " \"profile_image_url\": \"https://pbs.twimg.com/profile_images/434041506114985984/AdJ3cim3_normal.jpeg\", \n",
      " \"name\": \"mrquintopolous\", \n",
      " \"profile_text_color\": \"333333\", \n",
      " \"followers_count\": 45, \n",
      " \"protected\": false, \n",
      " \"profile_background_color\": \"9AE4E8\", \n",
      " \"favourites_count\": 193, \n",
      " \"time_zone\": \"Eastern Time (US & Canada)\", \n",
      " \"listed_count\": 3, \n",
      " \"id\": 16562593, \n",
      " \"screen_name\": \"mrquintopolous\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Lets create out api object w/ OAuth parameters\n",
    "api = twitter.Api(consumer_key='yp4wi4FASXbsRKa6JxYqzhUlH',\n",
    "                consumer_secret='Wkh1d5ygAOp4Bp65syFzHRN4xQsS8O4FvU3zHWosX8NXCqMpcl',\n",
    "                access_token_key='16562593-F6lRFe7iyoQEahezhPmaI64oInHZD0LNpcIbbq7Wy',\n",
    "                access_token_secret='weregYL8n6DI7yZy9pkizIJ78rH2GY02Do9jvpTe7rCey')\n",
    "\n",
    "user = api.GetUser(screen_name='mrquintopolous')\n",
    "print json.dumps(user.AsDict(), indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my personal twitter account. Lets get my list of followers (in some sense a depth-first search going on level deep):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "following = api.GetFriendIDs(screen_name='mrquintopolous')\n",
    "pickle.dump(following, open(\"following1\", \"wb\"))\n",
    "following = pickle.load(open(\"following1\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a script to help further crawl my followers, which can be found on [github](https://github.com/jquacinella/IndependentStudy/blob/master/Week2/crawlFollowing.py). The script needs to take into account timeouts and API limtis from twitter. To help speed up the process, I ran the script in parallel using two different sets of API keys. Annoying but effective. Here I will merge their results and look to import the data into an igraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the results from twitter\n",
    "following_depth_part1 = pickle.load(open('following_depth2.part1', 'rb'))\n",
    "following_depth_part2 = pickle.load(open('following_depth2.part2', 'rb'))\n",
    "following_depth_part1.update(following_depth_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct an igraph from the crawl\n",
    "from igraph import *\n",
    "\n",
    "# Create empty graph\n",
    "twitter_graph = Graph()\n",
    "\n",
    "# Add vertices to the graph\n",
    "me = '16562593'\n",
    "following = [str(follow) for follow in following]\n",
    "twitter_graph.add_vertices(following)\n",
    "twitter_graph.add_vertex(name=me) # me\n",
    "\n",
    "for follower in following:\n",
    "    try:\n",
    "        twitter_graph.add_edge(follower, me)\n",
    "    except Exception as e:\n",
    "        print follower\n",
    "        raise e\n",
    "\n",
    "for follower, following_depth2 in following_depth_part1.iteritems():\n",
    "    twitter_graph.add_vertex(name=str(follower))\n",
    "    try:\n",
    "        twitter_graph.add_vertices( [str(f) for f in following_depth2] )\n",
    "        twitter_graph.add_edges( [(str(f), str(follower)) for f in following_depth2 ] )\n",
    "    except Exception as e:\n",
    "        print f\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump(twitter_graph, open(\"twitter_graph\", \"wb\"))\n",
    "#twitter_graph = pickle.load(open(\"twitter_graph\", \"rb\"))\n",
    "layout = twitter_graph.layout(\"large\")\n",
    "plot(twitter_graph, layout = layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will not print out simple because it takes to long to calculate the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Week2/twitter_vertices\n",
      "PROGRESS: Parsing completed. Parsed 0 lines in 0.070235 secs.\n",
      "Insufficient number of rows to perform type inference"
     ]
    }
   ],
   "source": [
    "twitter_edges_f = open('twitter_egdes', 'w')\n",
    "twitter_vertices_f = open('twitter_vertices', 'w')\n",
    "\n",
    "me = '16562593'\n",
    "twitter_vertices_f.write(\"%s\" % me)\n",
    "for f in following:\n",
    "    twitter_vertices_f.write(\"%s\" % f)\n",
    "    twitter_edges_f.write(\"%s,%s\" % (f, me))\n",
    "\n",
    "\n",
    "for follower, following_depth2 in following_depth_part1.iteritems():\n",
    "    twitter_vertices_f.write(\"%s\" % follower)\n",
    "     \n",
    "    for f in following_depth2:\n",
    "        twitter_vertices_f.write(\"%s\" % str(f))\n",
    "        twitter_edges_f.write(\"%s,%s\" % (f, follower))\n",
    "        #twitter_graph.add_edges( [(str(f), str(follower)) for f in following_depth2 ] )\n",
    "\n",
    "# Load Data\n",
    "# Hide some silly output\n",
    "import logging\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# Import everything we need\n",
    "import graphlab as gl\n",
    "\n",
    "twitter_vertices = gl.SFrame.read_csv('twitter_vertices')\n",
    "twitter_edges = gl.SFrame.read_csv('twitter_egdes')\n",
    "\n",
    "# Create graph\n",
    "g_twitter = gl.SGraph()\n",
    "g_twitter = g_twitter.add_vertices(vertices=twitter_vertices, vid_field='name')\n",
    "g_twitter = g_twitter.add_edges(edges=twitter_edges, src_field='src', dst_field='dst')\n",
    "#g_twitter = g_twitter.add_edges(edges=kite_edges, src_field='dst', dst_field='src')\n",
    "\n",
    "# Visualize graph?\n",
    "gl.canvas.set_target('ipynb')\n",
    "g_twitter.show(vlabel=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Textual Analysis of Tweets from Political ThinkTanks\n",
    "\n",
    "In this part, I will download the tweet streams from different political 'think tanks' and perform a simple frequency analysis to see if there are any insights we can derive about the political leanings of these institutions. As an example of what tweet strams I will parse:\n",
    "\n",
    "- https://twitter.com/fairmediawatch - Fairness and Accuracy In Reporting\n",
    "- https://twitter.com/AccuracyInMedia - Accuracy In Media\n",
    "- https://twitter.com/ips_dc - Institute for Policy Studies\n",
    "- https://twitter.com/heritage - Heritage Foundation\n",
    "\n",
    "## 3.1 - Prep Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+----------------------------------------------+\n",
      "| Tweet Status                                                 | Expanded URLs                                |\n",
      "+--------------------------------------------------------------+----------------------------------------------+\n",
      "| That most US terrorists aren't Muslim \"may come as a         | http://bit.ly/1J7XVYL                        |\n",
      "| surprise\"--especially if you rely on corporate media.        |                                              |\n",
      "| http://t.co/J5bn1tQzRY                                       |                                              |\n",
      "| Baltimore \"gang threat\" swallowed by media was found to be   | http://bit.ly/1RxKvr6                        |\n",
      "| \"non-credible\" by FBI. @Vice @AdamJohnsonNYC                 |                                              |\n",
      "| http://t.co/4kZSXwnRka                                       |                                              |\n",
      "| Downplaying right's role in radicalizing Dylann Roof,        | http://bit.ly/1IeDNFQ                        |\n",
      "| corporate media preferred to pathologize him.                |                                              |\n",
      "| @BenjaminNorton http://t.co/am9Osetsnx                       |                                              |\n",
      "| How @PolitiFact manages to decide that it's \"mostly false\"   | http://bit.ly/1Jj6xP0                        |\n",
      "| to say that mass violence happens more frequently in US.     |                                              |\n",
      "| http://t.co/ZucxwZWcBt                                       |                                              |\n",
      "| &gt;@SaharaReporters covered #Charleston right: naming the   | http://bit.ly/1GxGlJI                        |\n",
      "| crime as \"terrorism\" &amp; focusing on killing of state      |                                              |\n",
      "| senator. http://t.co/mXg9lq9y5X                              |                                              |\n",
      "| \"I remember being a white male his age\": WaPo's @PBump,      | http://bit.ly/1eEVRO3                        |\n",
      "| arguing for not calling Dylann Roof a \"terrorist.\"           |                                              |\n",
      "| http://t.co/nsbPBQXrbX                                       |                                              |\n",
      "| Day after Boston bombing, 34% of newspaper stories referred  | http://bit.ly/1dVhCIv                        |\n",
      "| to \"terrorism\"; day after Charleston, 7% did so.             |                                              |\n",
      "| http://t.co/QzSfuTy3Aq                                       |                                              |\n",
      "| Women's sports are booming--but TV covers them less than it  | http://bit.ly/1K1UMLt                        |\n",
      "| did 10, 15, 20 or 25 years ago. @ProfCooky                   |                                              |\n",
      "| http://t.co/KBBCEbVO8R                                       |                                              |\n",
      "| CounterSpin talks to @KBZeese about corporate media blackout | http://bit.ly/1dOi4It                        |\n",
      "| of popular resistance to #TPP. http://t.co/jpzcrZp7G4        |                                              |\n",
      "| &gt;@NYTDavidBrooks says Mexican economy up, immigration     | http://bit.ly/1H1Z7zk                        |\n",
      "| down since NAFTA. Wrong &amp; wrong. http://t.co/ARTCDGmy81  |                                              |\n",
      "| http://t.co/4X6xffWThU                                       |                                              |\n",
      "| How corporate media treat @SenSanders like a sideshow.       | http://bit.ly/1H21kLb                        |\n",
      "| @AdamJohnsonNYC http://t.co/korGR4ihwa                       |                                              |\n",
      "| &gt;@NYTimes blames California drought on avocado-eaters--   | http://bit.ly/1InxawC                        |\n",
      "| reducing environmental crisis to consumer product choices.   |                                              |\n",
      "| http://t.co/hdSP9MEaj2                                       |                                              |\n",
      "| &gt;@NYTDavidBrooks' fact-challenged defense of TPP shows    | http://bit.ly/1H1Z7zk                        |\n",
      "| desperation of 1 Percenters: @DeanBaker13                    |                                              |\n",
      "| http://t.co/ARTCDGmy81                                       |                                              |\n",
      "| \"Torture victims are...barred from testifying about what the | http://bit.ly/1TuZCVP                        |\n",
      "| CIA did to them--torture architects are TV pundits.\"         |                                              |\n",
      "| http://t.co/60gNullIe2                                       |                                              |\n",
      "| Jane Brody's column on GMO food was slightly more convincing | http://nyti.ms/1JW9BPKhttp://nyti.ms/1R5LJcE |\n",
      "| http://t.co/wLZymRWTdO before the corrections.               |                                              |\n",
      "| http://t.co/G99uqlPhjs                                       |                                              |\n",
      "| Chris Cilizza @TheHyperFix thinks if you knew what you       | http://bit.ly/1JOOQ8k                        |\n",
      "| wanted, you'd want Lindsey Graham. http://t.co/FnnamT8x49    |                                              |\n",
      "| CounterSpin talks to Jessa Wilcox of @VeraInstitute about    | http://bit.ly/1Fdb2CX                        |\n",
      "| media misinformation on solitary confinement.                |                                              |\n",
      "| http://t.co/QtH8vwaOPV                                       |                                              |\n",
      "| \"Making stuff up for the cause always sells in official      | http://bit.ly/1I3OmqH                        |\n",
      "| Washington\": @DeanBaker13 on #TPP http://t.co/Mx8mhNb2oA     |                                              |\n",
      "| Josmar Trujillo of @AgainstBratton on how NYC media spin     | http://bit.ly/1BZqnXA                        |\n",
      "| crime stats to promote police power. http://t.co/2rqgwTDoUp  |                                              |\n",
      "| You’re 63% less likely to be murdered in de Blasio’s         | http://bit.ly/1BZqnXA                        |\n",
      "| Manhattan than Giuliani’s--but @NYPost won't say that.       |                                              |\n",
      "| http://t.co/2rqgwTDoUp                                       |                                              |\n",
      "| CNN hopes its “trustworthiness” will make its “news-like     | http://bit.ly/1JA0u8F                        |\n",
      "| content” for advertisers valuable.  http://t.co/EcGUelCIIo   |                                              |\n",
      "| http://t.co/yR3T21RQol                                       |                                              |\n",
      "| &gt;@CNN has a new unit to produce “news-like content on     | http://bit.ly/1JA0u8F                        |\n",
      "| behalf of advertisers.” Its name? \"Courageous.\"              |                                              |\n",
      "| http://t.co/EcGUelCIIo                                       |                                              |\n",
      "| &gt;@AP calls police violence \"incidents involving...black   | http://bit.ly/1G7CnKI                        |\n",
      "| suspects\"--as if everyone shot by cops is a \"suspect.\"       |                                              |\n",
      "| http://t.co/BJhIE82VZ4                                       |                                              |\n",
      "| &gt;@MaxBoot responds to @GGreenwald: My error only proves   | http://bit.ly/1GjbtRghttp://bit.ly/1QCIXeY   |\n",
      "| me right: http://t.co/bDeyycmFZU Still hasn't corrected      |                                              |\n",
      "| post: http://t.co/HlQhOG206k                                 |                                              |\n",
      "| Still no correction from @Commentary of @MaxBoot's           | http://bit.ly/1It2WxF                        |\n",
      "| indisputable error in his essay on #EdwardSnowden.           |                                              |\n",
      "| http://t.co/wy7GAwNZIm                                       |                                              |\n",
      "| Oddly, @MaxBoot wrongly says Snowden ignored Russia in his   | http://bit.ly/1It2WxF                        |\n",
      "| @NYTimes op-ed--then refuses to correct his mistake.         |                                              |\n",
      "| http://t.co/wy7GAwNZIm                                       |                                              |\n",
      "| \"NSA Data Collection Ended\": How @USAToday's headline is     | http://bit.ly/1QxyJfP                        |\n",
      "| wrong four different ways. @JNaureckas                       |                                              |\n",
      "| http://t.co/z6w4YSTOo5                                       |                                              |\n",
      "| On CounterSpin, @Sueudry of @DefendDissent: Is USA Freedom   | http://bit.ly/1FyhghZ                        |\n",
      "| Act a step toward halting mass surveillance or not?          |                                              |\n",
      "| http://t.co/A9mdxV2weg                                       |                                              |\n",
      "| &gt;@tomfriedman uses Baltimore Uprising as excuse to run a  | http://bit.ly/1JrlTiR                        |\n",
      "| commercial for his wife’s charter school. @AdamJohnsonNYC    |                                              |\n",
      "| http://t.co/ekWfTAPvZt                                       |                                              |\n",
      "| If people are already having trouble affording homes, why    | http://bit.ly/1FU8VaI                        |\n",
      "| does @NYTimes want them to be more expensive? @DeanBaker13   |                                              |\n",
      "| http://t.co/9M2Ca7IYzd                                       |                                              |\n",
      "| The popular issues that make Sanders \"unelectable,\"          | http://bit.ly/1AM3UlI                        |\n",
      "| according to @NYTimes. @JNaureckas on @CommonDreams:         |                                              |\n",
      "| http://t.co/XMGOjZv8Xm                                       |                                              |\n",
      "| Janine Jackson demolishes white male media self-pity in 1    | http://bit.ly/1cwc8Tq                        |\n",
      "| minute 56 seconds. @BestOfTheLeft @MichaelWolffNYC           |                                              |\n",
      "| http://t.co/oUdTYyjcH5                                       |                                              |\n",
      "| &gt;@NYTimes suggests Iowa Dems will find #BernieSanders     | http://bit.ly/1M1f1HU                        |\n",
      "| \"unelectable\"--because he favors taxing rich, like 3/5 of    |                                              |\n",
      "| US. http://t.co/9zvmynFEiE                                   |                                              |\n",
      "| On @FaceTheNation, \"all segments of American life\" means GOP | http://bit.ly/1KIFfBJ                        |\n",
      "| + CIA. @JNaureckas on @BobSchieffer's last show.             |                                              |\n",
      "| http://t.co/ntg5GJ5OJo                                       |                                              |\n",
      "| This week's CounterSpin: @BartNaylor on corporate media's    | http://bit.ly/1FI5bsR                        |\n",
      "| dangerous indifference to questions of criminal banks.       |                                              |\n",
      "| http://t.co/Kx8VpYZVtb                                       |                                              |\n",
      "| A Fed interest rate hike will really impact your life. But   | http://bit.ly/1GGQRCX                        |\n",
      "| @NPR wants you to worry about the deficit. @DeanBaker13:     |                                              |\n",
      "| http://t.co/BJ9cPpa33G                                       |                                              |\n",
      "| Teddy White: The \"strain\" of journalism is writing as if     | http://bit.ly/1Hynu1I                        |\n",
      "| \"what is true and what is fraud\" can't be distinguished.     |                                              |\n",
      "| http://t.co/n9zcu20RJp                                       |                                              |\n",
      "| &gt;@NYTOpinion's deceptive case for Democrats being \"too    | http://bit.ly/1cl9Gz0                        |\n",
      "| far left\": @JNaureckas on @CommonDreams                      |                                              |\n",
      "| http://t.co/3AkOwHg5ba                                       |                                              |\n",
      "| &gt;@NYTOpinion argues Obama to left of Clinton based on     | http://bit.ly/1cl9Gz0                        |\n",
      "| deficit spending. In reality, Obama did more to reduce       |                                              |\n",
      "| deficit. http://t.co/3AkOwHg5ba                              |                                              |\n",
      "| Are Democrats \"Too Far Left?\" Of course @NYTOpinion answers  | http://bit.ly/1RoLOKF                        |\n",
      "| yes--but still surprises with degree of dishonesty.          |                                              |\n",
      "| http://t.co/cvzb3QOlUW                                       |                                              |\n",
      "| If media really wanted to prevent dissemination of ISIS      | http://bit.ly/1dybGFE                        |\n",
      "| propaganda, they could stop disseminating ISIS propaganda.   |                                              |\n",
      "| http://t.co/MUhbvB0Ap2                                       |                                              |\n",
      "| &gt;@NYTimes covered Cruz, Paul, Rubio campaign kick-offs on | http://bit.ly/1KzJhwd                        |\n",
      "| Page 1. #BernieSanders? A21. @CJR @Sulliview                 |                                              |\n",
      "| http://t.co/SnHKzl86g9                                       |                                              |\n",
      "| The most effective vehicle for disseminating ISIS propaganda | http://bit.ly/1HJnJg0                        |\n",
      "| is the Western media outrage machine. @AdamJohnsonNYC        |                                              |\n",
      "| http://t.co/jZ0u9UuZJo                                       |                                              |\n",
      "| &gt;@NYTimes describes workplace repression of women as      | http://bit.ly/1F8UBYo                        |\n",
      "| “unintended” impact of family-friendly policies. @clairecm   |                                              |\n",
      "| http://t.co/EC5xdLLSpi                                       |                                              |\n",
      "+--------------------------------------------------------------+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Lets load up the Twitter API\n",
    "import twitter\n",
    "import prettytable\n",
    "\n",
    "# Grab FAIR's tweet stream\n",
    "#\n",
    "# NOTE: do not include retweets, too many dupes (though for text analysis this might be a \n",
    "#      way to weigh more heavily text from tweets that are being retweeted by the account)\n",
    "statuses = api.GetUserTimeline(screen_name='fairmediawatch', count=500, include_rts=False)\n",
    "\n",
    "# Create a pretty table of tweet contents and any expanded urls\n",
    "pt = prettytable.PrettyTable([\"Tweet Status\", \"Expanded URLs\"])\n",
    "pt.align[\"Tweet Status\"] = \"l\" # Left align city names\n",
    "pt.align[\"Expanded URLs\"] = \"l\" # Left align city names\n",
    "pt.max_width = 60 \n",
    "pt.padding_width = 1 # One space between column edges and contents (default)\n",
    "\n",
    "# Add rows to the pretty table\n",
    "for status in statuses:\n",
    "    pt.add_row([status.text, \"\".join([url.expanded_url for url in status.urls]) ])\n",
    "\n",
    "# Lets see the results!\n",
    "print pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3.2 - Getting Recent Tweets from All Accounts\n",
    "\n",
    "Lets get the tweets for all the accounts, and store them in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of accounts to process, and our results dict\n",
    "accounts = ['fairmediawatch', 'AccuracyInMedia', 'ips_dc', 'heritage']\n",
    "allStatuses = { }\n",
    "\n",
    "# For each account, query tiwtter for top tweets\n",
    "for account in accounts:\n",
    "    allStatuses[account] = api.GetUserTimeline(screen_name=account, count=500, include_rts=False)\n",
    "\n",
    "# Save results\n",
    "import pickle\n",
    "pickle.dump( allStatuses, open( \"allStatuses\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NTLK, we will process the tweets ands split them into words (while filtering for stopwords) and storing all hastag mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words = { }\n",
    "hashtags = { }\n",
    "counters = { \"words\": { }, \"hashtags\": { } }\n",
    "\n",
    "for account in accounts:\n",
    "    words[account] = [ w.lower() for t in allStatuses[account] for w in t.text.split() if w.lower() not in stopwords.words('english') ]\n",
    "    counters[\"words\"][account] = Counter(words[account])\n",
    "    \n",
    "    hashtags[account] = [ hashtag.text.lower() for status in allStatuses[account] for hashtag in status.hashtags ]\n",
    "    counters[\"hashtags\"][account] = Counter(hashtags[account])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out the prominent words from each account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ips_dc\n",
      "+----------------+-------+\n",
      "| Word           | Count |\n",
      "+----------------+-------+\n",
      "| u.s.           |    21 |\n",
      "| via            |    12 |\n",
      "| -              |    12 |\n",
      "| #andstillirise |    12 |\n",
      "| #tpp           |    12 |\n",
      "| women          |     9 |\n",
      "| black          |     9 |\n",
      "| &amp;          |     9 |\n",
      "| labor          |     7 |\n",
      "| world          |     7 |\n",
      "| corporations   |     6 |\n",
      "| ↑              |     5 |\n",
      "| leaders        |     5 |\n",
      "| movement       |     5 |\n",
      "| could          |     5 |\n",
      "| americans      |     5 |\n",
      "| #stopfasttrack |     5 |\n",
      "| take           |     5 |\n",
      "| justice        |     5 |\n",
      "| w/             |     5 |\n",
      "+----------------+-------+\n",
      "\n",
      "fairmediawatch\n",
      "+------------------------+-------+\n",
      "| Word                   | Count |\n",
      "+------------------------+-------+\n",
      "| media                  |     9 |\n",
      "| corporate              |     5 |\n",
      "| &gt;@nytimes           |     4 |\n",
      "| @jnaureckas            |     4 |\n",
      "| @adamjohnsonnyc        |     4 |\n",
      "| still                  |     3 |\n",
      "| @deanbaker13           |     3 |\n",
      "| isis                   |     3 |\n",
      "| talks                  |     2 |\n",
      "| counterspin            |     2 |\n",
      "| tv                     |     2 |\n",
      "| far                    |     2 |\n",
      "| says                   |     2 |\n",
      "| \"too                   |     2 |\n",
      "| @nytimes               |     2 |\n",
      "| right:                 |     2 |\n",
      "| us.                    |     2 |\n",
      "| male                   |     2 |\n",
      "| http://t.co/artcdgmy81 |     2 |\n",
      "| want                   |     2 |\n",
      "+------------------------+-------+\n",
      "\n",
      "AccuracyInMedia\n",
      "+-------------------+-------+\n",
      "| Word              | Count |\n",
      "+-------------------+-------+\n",
      "| #tcot             |   153 |\n",
      "| confederate       |    24 |\n",
      "| @hillaryclinton   |    21 |\n",
      "| state             |    19 |\n",
      "| liberal           |    19 |\n",
      "| obamacare         |    17 |\n",
      "| media             |    17 |\n",
      "| dept              |    16 |\n",
      "| flag              |    15 |\n",
      "| e-mails           |    15 |\n",
      "| watch:            |    13 |\n",
      "| obama's           |    11 |\n",
      "| clinton           |    11 |\n",
      "| battle            |    10 |\n",
      "| says              |    10 |\n",
      "| obama             |    10 |\n",
      "| get               |    10 |\n",
      "| jonathan          |     9 |\n",
      "| @hillaryclinton's |     9 |\n",
      "| admits            |     9 |\n",
      "+-------------------+-------+\n",
      "\n",
      "heritage\n",
      "+-------------------+-------+\n",
      "| Word              | Count |\n",
      "+-------------------+-------+\n",
      "| #obamacare        |    17 |\n",
      "| new               |    14 |\n",
      "| government        |    13 |\n",
      "| #exim             |    13 |\n",
      "| –                 |    10 |\n",
      "| congress          |    10 |\n",
      "| #scotus           |     9 |\n",
      "| court             |     8 |\n",
      "| obama             |     8 |\n",
      "| #ndaa             |     8 |\n",
      "| king              |     7 |\n",
      "| #forfeiturereform |     7 |\n",
      "| @tedcruz          |     7 |\n",
      "| make              |     7 |\n",
      "| v.                |     7 |\n",
      "| obamacare         |     6 |\n",
      "| burwell           |     6 |\n",
      "| actually          |     6 |\n",
      "| insurance         |     6 |\n",
      "| could             |     6 |\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for account in counters[\"words\"]:\n",
    "    pt = prettytable.PrettyTable(field_names=['Word', 'Count'])\n",
    "    [ pt.add_row(kv) for kv in counters[\"words\"][account].most_common()[:20] ]\n",
    "    pt.align['Word'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    print account\n",
    "    print pt\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All lets print out the most prominent hashtags from all the accounts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ips_dc\n",
      "+-------------------------+-------+\n",
      "| Hashtag                 | Count |\n",
      "+-------------------------+-------+\n",
      "| tpp                     |    19 |\n",
      "| andstillirise           |    12 |\n",
      "| stopfasttrack           |     5 |\n",
      "| fasttrack               |     4 |\n",
      "| studentdebt             |     4 |\n",
      "| fightfor15              |     3 |\n",
      "| popefrancis             |     3 |\n",
      "| notpp                   |     3 |\n",
      "| ttip                    |     2 |\n",
      "| 1u                      |     2 |\n",
      "| blackworkersmatter      |     2 |\n",
      "| isis                    |     2 |\n",
      "| blackworkingwomenmatter |     1 |\n",
      "| charleston              |     1 |\n",
      "| wallstreet              |     1 |\n",
      "| climatechange           |     1 |\n",
      "| estatetax               |     1 |\n",
      "| g7summit                |     1 |\n",
      "| congress                |     1 |\n",
      "| domesticworkersday      |     1 |\n",
      "+-------------------------+-------+\n",
      "\n",
      "fairmediawatch\n",
      "+---------------+-------+\n",
      "| Hashtag       | Count |\n",
      "+---------------+-------+\n",
      "| berniesanders |     2 |\n",
      "| tpp           |     2 |\n",
      "| charleston    |     1 |\n",
      "| edwardsnowden |     1 |\n",
      "+---------------+-------+\n",
      "\n",
      "AccuracyInMedia\n",
      "+-------------------+-------+\n",
      "| Hashtag           | Count |\n",
      "+-------------------+-------+\n",
      "| tcot              |   153 |\n",
      "| scotus            |     7 |\n",
      "| kingvburwell      |     7 |\n",
      "| benghazi          |     4 |\n",
      "| prolife           |     4 |\n",
      "| laudatosi         |     4 |\n",
      "| 2a                |     2 |\n",
      "| catholic          |     1 |\n",
      "| sarcasm           |     1 |\n",
      "| education         |     1 |\n",
      "| prayforcharleston |     1 |\n",
      "+-------------------+-------+\n",
      "\n",
      "heritage\n",
      "+------------------+-------+\n",
      "| Hashtag          | Count |\n",
      "+------------------+-------+\n",
      "| obamacare        |    21 |\n",
      "| exim             |    13 |\n",
      "| scotus           |     9 |\n",
      "| forfeiturereform |     8 |\n",
      "| ndaa             |     8 |\n",
      "| religiousliberty |     4 |\n",
      "| obama            |     3 |\n",
      "| tpa              |     3 |\n",
      "| trade            |     2 |\n",
      "| netneutrality    |     2 |\n",
      "| kelo             |     2 |\n",
      "| esa              |     2 |\n",
      "| internetfreedom  |     2 |\n",
      "| government       |     2 |\n",
      "| magnacarta       |     2 |\n",
      "| inequality       |     2 |\n",
      "| endexim          |     2 |\n",
      "| htf              |     1 |\n",
      "| churchill        |     1 |\n",
      "| poor             |     1 |\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for account in counters[\"hashtags\"]:\n",
    "    pt = prettytable.PrettyTable(field_names=['Hashtag', 'Count'])\n",
    "    [ pt.add_row(kv) for kv in counters[\"hashtags\"][account].most_common()[:20] ]\n",
    "    pt.align['Hashtag'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    print account\n",
    "    print pt\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, things make sense. The two 'left leaning' accounts list hashtags like \"tpp, stopfasttrack, fasttrack, studentdebt, fightfor15, notpp, blackworkersmatter, blackworkingwomenmatter, berniesanders\", which are all left leaning. FAIR used words like 'corporate' and 'media', which make sensesince they are a media-watchdog.\n",
    "\n",
    "The two right leaning accounts mention typical phrases and hastags from that side, including the most prominent conservative hastag #tcot, but also \"scotus, kingvburwell, benghazi, prolife, obamacare, exim, forfeiturereform, ndaa, religiousliberty, obama\", all very prominent conservative issues.\n",
    "\n",
    "## Future Research\n",
    "\n",
    "- Stemming\n",
    "- Extend to bi and tri-grams\n",
    "- Link words to topics or linked open data\n",
    "- Sentiment analysis\n",
    "- Topic Modeling\n",
    "- Crawl articles linked to by tweets and use boilerpipe to extend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
