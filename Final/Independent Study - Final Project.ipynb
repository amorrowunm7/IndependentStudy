{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - James Quacinella\n",
    "\n",
    "Fo the final project, I will look at the follower network of one of the think tank Twitter account and perform clustering to find groups of associated accounts. Looking at the clusters, I hope to identify what joins them by performing some NLP tasks on the account's profile contents.\n",
    "\n",
    "## Step 1 - Crawl Twitter for Followers\n",
    "\n",
    "The next section of code does not run in the notebook, but is a copy of the crawler code created for this project. It will take a single account, get the first level followers, and then grab the 'second-level' followers. Those second level follower are only added if they were nodes in the first level (so we focus on the main account, not other accounts tangentially related)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import graphlab as gl\n",
    "import pickle\n",
    "import twitter\n",
    "import logging\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "### Setup a console and file logger\n",
    "\n",
    "logger = logging.getLogger('crawler')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('crawler.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "### Setup signals to make sure API calls only take 60s at most\n",
    "\n",
    "from functools import wraps\n",
    "import errno\n",
    "import os\n",
    "import signal\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout(seconds=60, error_message=os.strerror(errno.ETIME)):\n",
    "    def decorator(func):\n",
    "        def _handle_timeout(signum, frame):\n",
    "            raise TimeoutError(error_message)\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
    "            signal.alarm(seconds)\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            return result\n",
    "\n",
    "        return wraps(func)(wrapper)\n",
    "\n",
    "    return decorator\n",
    "\n",
    "@timeout()\n",
    "def getFollowers(api, follower):\n",
    "    ''' Function that will get a user's list of followers from an api object. \n",
    "    NOTE: the decorator ensures that this only runs for 60s at most. '''\n",
    "    # return api.GetFollowerIDs(follower)\n",
    "    return api.GetFriendIDs(follower)\n",
    "\n",
    "\n",
    "### Twitter API\n",
    "\n",
    "# Lets create our list of api OAuth parameters\n",
    "API_TOKENS = [\n",
    "    {\"consumer_key\": 'yp4wi4FASXbsRKa6JxYqzhUlH',\n",
    "    \"consumer_secret\": 'Wkh1d5ygAOp4Bp65syFzHRN4xQsS8O4FvU3zHWosX8NXCqMpcl',\n",
    "    \"access_token_key\": '16562593-F6lRFe7iyoQEahezhPmaI64oInHZD0LNpcIbbq7Wy',\n",
    "    \"access_token_secret\": 'weregYL8n6DI7yZy9pkizIJ78rH2GY02Do9jvpTe7rCey',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'NsNYFG9LtZV2XMyigPaCKVyVz',\n",
    "    \"consumer_secret\": '4J1vlowybipqXnSrKgLBvmzPmwqx71uHN32noljTgDLS2xQNfI',\n",
    "    \"access_token_key\": '16562593-NCuQWVnpzcnB55w7VLdoCkdobdUQBRDJKjIPXAksP',\n",
    "    \"access_token_secret\": 'nX9OksrYQxj0jBXYJTkUjlX5mZh4rZljfVRXtSM3Tjc8c',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'ZcAMGe2MUcnTO9ATCIo563SHN',\n",
    "    \"consumer_secret\": 'dJAB7mBfoYyx27Yccbmzz98GtNigAA67Ish9Y1NjN2wNznciM1',\n",
    "    \"access_token_key\": '16562593-AmaoKVLEYL3o8rVUS3b6u4PUbVPTI6BPsyaqCdwxY',\n",
    "    \"access_token_secret\": '8pjYJCFWTErJlb2WSkLwsYNoptVazQQs95JAvIU8JApUA',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'avZpjObqQN9vue2Y4gu9zIF9X',\n",
    "    \"consumer_secret\": 'Ka6WCj3fyon5yGgf5YJIIl8nVcLcUh5YT99N58qy8qv4kfaMbc',\n",
    "    \"access_token_key\": '16562593-VNuGD09Cr29ZlzNCWnV5MOujU7PsexSwfTgfKQNqC',\n",
    "    \"access_token_secret\": '9P3hB3qDb9zPDFCUhWU16N4CMXPwHacl6HJbCc0EuGj7s',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'sQ9H5NKteroNZSWvIrkSWvXR0',\n",
    "    \"consumer_secret\": 'lC0ttZKdIZhhJAE1I5RxMxdjpSiADQCVUnHS7LbtfVmI2pz2F2',\n",
    "    \"access_token_key\": '16562593-4LOk7QkXWD0boF01BmZ6NP2oPtHmDZ1OVJ883aANG',\n",
    "    \"access_token_secret\": 'JJ85qMqzVowN1KdQ6w4YlhJB9YF9eWbw6SGbxQoU6gvne',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'DHppZ2LG3iYj8vEx7ibRRLN35',\n",
    "    \"consumer_secret\": 'wdTQeyp7ZNDN7ne40IriRw7Ah1J8cAi2OIlw4MVtgpq5MMKjYE',\n",
    "    \"access_token_key\": '16562593-WN8zvEWAxVfJPrneMwUjDoVQw0geuLckOOJqFimsC',\n",
    "    \"access_token_secret\": 'ZgVi2onPB3RPGtRmPBs6QXymIMgXwJHUOQycesp64S0Hp',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'lIgtfdkC2WmN7XAcicrGygQBp',\n",
    "    \"consumer_secret\": '2D9WIJN2MIPwFpMeIGcP6vWjQC8vvy7G5ZlHMSH1F1CsgWGKfz',\n",
    "    \"access_token_key\": '16562593-7lhPpeZNNAGoQQJnqcnTtBiGq1O52XMZ4CMeVqXiY',\n",
    "    \"access_token_secret\": 'WKRBQsr36MMB2EpCcZLr89ik0MSJfPoBORCKu9E1hw96I',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": '1XFu2urZzoMoC5sadXAjA7IoQ',\n",
    "    \"consumer_secret\": 'FrJOlHfNLp3M7ejJWiO5k74E9ai6L5EzQJ45HmlsUINbh8qUUi',\n",
    "    \"access_token_key\": '16562593-Texko6g7VyCwhNUfxBDoJKJl4058hpvQkqAYWRKpi',\n",
    "    \"access_token_secret\": 'ISZCTvN6bYJVaJ3Z2iidQObTzE2pxkINBLi0WWe9Ab2Zv',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'r8Bvdm6I8QrRPuVzP4VtRYpqd',\n",
    "    \"consumer_secret\": 'CzA8u8M8nDiDCCrSzCsXpR3SyTGCaLppDWbdTxSg78ZKgtKkhh',\n",
    "    \"access_token_key\": '16562593-I3l0ZSmfZbMxIQ2NbiiM2eDMA4KNzFmFBeUkWxunR',\n",
    "    \"access_token_secret\": '9HkILP4kSMF0hgvsB126jpoUzsRXETYMlSM0YSKb2yMJH',\n",
    "    \"requests_timeout\": 60},\n",
    "\n",
    "    {\"consumer_key\": 'NmMjfP1Zt3n2VDZ15X7SDGM6G',\n",
    "    \"consumer_secret\": 'j9JBx7HUbMpcDnFteiIAAgHSoA8idlqQ20A1xbvnMrqMrOHQ1n',\n",
    "    \"access_token_key\": '16562593-zUNyMUdO9JnSIstmTrqdyHHmX2lpv9NqkQxGC8faP',\n",
    "    \"access_token_secret\": 'DEeHvLjTXlxNGmqDntXOK0cJCX08cnpg0btoRXWATW3X2',\n",
    "    \"requests_timeout\": 60}\n",
    "]\n",
    "\n",
    "# Now create a list of twitter API objects\n",
    "apis = []\n",
    "for token in API_TOKENS:\n",
    "    apis.append( twitter.Api(consumer_key=token['consumer_key'],\n",
    "                    consumer_secret=token['consumer_secret'],\n",
    "                    access_token_key=token['access_token_key'],\n",
    "                    access_token_secret=token['access_token_secret'],\n",
    "                    requests_timeout=60))\n",
    "\n",
    "\n",
    "# The account id / screen name we want followers from\n",
    "account_screen_name = 'fairmediawatch'\n",
    "account_id = '54679731'\n",
    "\n",
    "# Keep track of nodes connected to account, and all edges we need in the graph\n",
    "nodes = set()\n",
    "edges = defaultdict(set)\n",
    "\n",
    "\n",
    "# Try to load first level followers from pickle;\n",
    "# otherwise, generate them from a single API call and save via pickle\n",
    "try:\n",
    "    logger.info(\"Loading followers for %s\" % account_screen_name)\n",
    "    f = open(\"following1\", \"rb\")\n",
    "    following = pickle.load(f)\n",
    "except Exception as e:\n",
    "    logger.info(\"Failed. Generating followers for %s\" % account_screen_name)\n",
    "    following = api.GetFriendIDs(screen_name=account_screen_name)\n",
    "    pickle.dump(following, open(\"following1\", \"wb\"))\n",
    "\n",
    "# Try to load the nodes and first level edges from pickle;\n",
    "# otherwise generate them from the 'following' list and save\n",
    "try:\n",
    "    logger.info(\"Loading nodes and edges for depth = 1, for %s\" % account_screen_name)\n",
    "    n = open(\"nodes.follow1.set\", \"rb\")\n",
    "    e = open(\"edges.follow1.dict\", \"rb\")\n",
    "    nodes = pickle.load(n)\n",
    "    edges = pickle.load(e)\n",
    "except Exception as e:\n",
    "    logger.info(\"Failed. Generating nodes and edges for depth = 1, for %s\" % account_screen_name)\n",
    "    for follower in following:\n",
    "        nodes.add(follower)\n",
    "        edges[account_id].add(follower)\n",
    "    pickle.dump(nodes, open(\"nodes.follow1.set\", \"wb\"))\n",
    "    pickle.dump(edges, open(\"edges.follow1.dict\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "### Crawling for Depth2\n",
    "\n",
    "\n",
    "# Index the api list, and start from the first api object\n",
    "api_idx = 0\n",
    "api = apis[api_idx]\n",
    "\n",
    "# Some accounts give us issues (either too many followers or no permissions)\n",
    "blacklist= [74323323, 43532023, 19608297, 25757924, 240369959, 173634807, 17008482, 142143804]\n",
    "api_updated = False\n",
    "\n",
    "# It is nice to start from a point in the list, instead of from the beginning\n",
    "starting_point = 142143804\n",
    "if starting_point:\n",
    "    starting_point_idx = following.index(starting_point)\n",
    "    following_iter = range(starting_point_idx, len(following))\n",
    "else:\n",
    "    following_iter = range(len(following))\n",
    "\n",
    "# Try loading second layer of followers from pickle, otherwise start from scratch\n",
    "try:\n",
    "    f = open(\"edges.follow2.dict\", \"rb\")\n",
    "    edges = pickle.load(f)\n",
    "    logger.info(\"Loaded edges.follow2 into memory!\")\n",
    "except Exception as e:\n",
    "    logger.info(\"Starting from SCRATCH: did not load edges.follow2 into memory!\")\n",
    "    pass\n",
    "\n",
    "# For each follower of the main account ...\n",
    "for follower_idx in following_iter:\n",
    "    follower = following[follower_idx]\n",
    "    success = False\n",
    "    \n",
    "    # ... check if they are on the blacklist; if so, skip\n",
    "    if follower in blacklist:\n",
    "        logger.info(\"Skipping due to blacklist\")\n",
    "        continue\n",
    "\n",
    "    # Otherwise, attempt to get list of their followers\n",
    "    followers_depth2_list = []\n",
    "    while not success:\n",
    "        try:\n",
    "            logger.info(\"Getting followers for follower %s\" % follower)\n",
    "            followers_depth2_list = getFollowers(api, follower)\n",
    "            success = True\n",
    "        except TimeoutError as e:\n",
    "            # If api call takes too long, move on\n",
    "            logger.info(\"Timeout after 60s for follower %d\" % follower)\n",
    "            success = True      # technically not a success but setting flag so next loop moves on\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # IF we get here, then we hit API limits\n",
    "            logger.info(\"API Exception %s; api-idx = %d\" % (str(e), api_idx))\n",
    "            \n",
    "            # Are we at the begining of api list? \n",
    "            # IF so, dump edges so far via pickle and sleep\n",
    "            if api_updated and api_idx % len(API_TOKENS) == 0 and api_idx >= len(API_TOKENS):\n",
    "                logger.info(\"Save edges to pickle file for follower = %s\" % follower)\n",
    "                pickle.dump(edges, open(\"edges.follow2.dict\", \"wb\"))\n",
    "                logger.info(\"Sleeping ...\")\n",
    "                time.sleep(60)\n",
    "                api_updated = False\n",
    "            # Otherwise, move on to the next api object and try again\n",
    "            else:\n",
    "                api_idx += 1\n",
    "                api = apis[api_idx % len(API_TOKENS)]\n",
    "                api_updated = True\n",
    "            \n",
    "    \n",
    "    # After getting the followers, find the intersection of those followers\n",
    "    # with those of the first-level followers and add to edge dict\n",
    "    if followers_depth2_list:\n",
    "        logger.info(\"Adding followers to the graph\")\n",
    "        edges[follower].update(nodes.intersection(followers_depth2_list))\n",
    "\n",
    "\n",
    "# Write out final list of edges via pickle\n",
    "logger.info(\"Save edges to pickle file for follower = %s\" % follower)\n",
    "pickle.dump(edges, open(\"edges.follow2.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running the above, lets just load everything via pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "n = open(\"nodes.follow1.set\", \"rb\")\n",
    "nodes = pickle.load(n)\n",
    "\n",
    "e = open(\"edges.follow2.dict\", \"rb\")\n",
    "edges = pickle.load(e)\n",
    "\n",
    "f = open(\"following1\", \"rb\")\n",
    "following = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 2 - Generate Graph from Crawl\n",
    "\n",
    "First, we generate CSV files so we can load data into GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hide some silly output\n",
    "import logging\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# Import everything we need\n",
    "import graphlab as gl\n",
    "\n",
    "# Generate CSVs from the previous crawl\n",
    "# TODO\n",
    "f = open('vertices.csv', 'w')\n",
    "f.write('id\\n')\n",
    "for node in nodes:\n",
    "    f.write(str(node) + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open('edges.csv', 'w')\n",
    "f.write('src,dst,relation\\n')\n",
    "for node, followers in edges.iteritems():\n",
    "    for follower in followers:\n",
    "        f.write('%s,%s,%s\\n' % (follower, node, 'follows'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us use these CSV files and load them into a graph object called g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Final/vertices.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.02126 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Final/vertices.csv\n",
      "PROGRESS: Parsing completed. Parsed 1106 lines in 0.020157 secs.\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Final/edges.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.114567 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /home/james/Development/Masters/IndependentStudy/Final/edges.csv\n",
      "PROGRESS: Parsing completed. Parsed 105006 lines in 0.087203 secs.\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "gvertices = gl.SFrame.read_csv('vertices.csv')\n",
    "gedges = gl.SFrame.read_csv('edges.csv')\n",
    "\n",
    "# Create graph\n",
    "g = gl.SGraph()\n",
    "g = g.add_vertices(vertices=gvertices, vid_field='id')\n",
    "g = g.add_edges(edges=gedges, src_field='src', dst_field='dst')\n",
    "g = g.add_edges(edges=gedges, src_field='dst', dst_field='src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to visualize the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canvas is accessible via web browser at the URL: http://localhost:48677/index.html\n",
      "Opening Canvas in default web browser.\n"
     ]
    }
   ],
   "source": [
    "# Visualize graph?\n",
    "gl.canvas.set_target('browser')\n",
    "g.show(vlabel=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like its too large of a graph to display. \n",
    "\n",
    "## Central / Important Nodes\n",
    "\n",
    "Lets use pagerank to find important nodes in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Counting out degree\n",
      "PROGRESS: Done counting out degree\n",
      "PROGRESS: +-----------+-----------------------+\n",
      "PROGRESS: | Iteration | L1 change in pagerank |\n",
      "PROGRESS: +-----------+-----------------------+\n",
      "PROGRESS: | 1         | 617.534               |\n",
      "PROGRESS: | 2         | 135.25                |\n",
      "PROGRESS: | 3         | 30.9247               |\n",
      "PROGRESS: | 4         | 8.64859               |\n",
      "PROGRESS: | 5         | 2.52531               |\n",
      "PROGRESS: | 6         | 0.885368              |\n",
      "PROGRESS: | 7         | 0.323184              |\n",
      "PROGRESS: | 8         | 0.126578              |\n",
      "PROGRESS: | 9         | 0.0503135             |\n",
      "PROGRESS: | 10        | 0.0203128             |\n",
      "PROGRESS: | 11        | 0.00825336            |\n",
      "PROGRESS: +-----------+-----------------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">__id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">pagerank</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">54679731</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.15893054698</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0816305339e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">59159771</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.73589508502</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.14342970437e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">169182727</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.68248985863</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.48873868332e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">16935292</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.98957011223</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.37281975531e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1947301</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.39339614539</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.60673869321e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">23839835</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.36113011846</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.93642112576e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">16076032</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.34163894719</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.78788532152e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10117892</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.96520683672</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.1042595571e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">16955991</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.84512060914</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.25983857795e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">478203018</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.40106898918</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.55084466292e-05</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\t__id\tint\n",
       "\tpagerank\tfloat\n",
       "\tdelta\tfloat\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-----------+---------------+-------------------+\n",
       "|    __id   |    pagerank   |       delta       |\n",
       "+-----------+---------------+-------------------+\n",
       "|  54679731 | 7.15893054698 |  2.0816305339e-05 |\n",
       "|  59159771 | 5.73589508502 | 5.14342970437e-06 |\n",
       "| 169182727 | 5.68248985863 | 2.48873868332e-05 |\n",
       "|  16935292 | 4.98957011223 | 3.37281975531e-05 |\n",
       "|  1947301  | 4.39339614539 | 1.60673869321e-06 |\n",
       "|  23839835 | 4.36113011846 | 1.93642112576e-05 |\n",
       "|  16076032 | 4.34163894719 | 7.78788532152e-06 |\n",
       "|  10117892 | 3.96520683672 |  4.1042595571e-06 |\n",
       "|  16955991 | 3.84512060914 | 1.25983857795e-05 |\n",
       "| 478203018 | 3.40106898918 | 2.55084466292e-05 |\n",
       "+-----------+---------------+-------------------+\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = gl.pagerank.create(g)\n",
    "pr.get('pagerank').topk(column_name='pagerank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  The Graph to iGraph\n",
    "\n",
    "Next we will load the graph data into igrpah and perform clustering to find communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from igraph import *\n",
    "\n",
    "# Create empty graph\n",
    "twitter_graph = Graph(directed=False)\n",
    "\n",
    "# Setup the nodes\n",
    "for node in nodes:\n",
    "    if isinstance(node, int):\n",
    "        twitter_graph.add_vertex(name=str(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the edges\n",
    "for user in edges:\n",
    "    for follower in edges[user]:\n",
    "        try:\n",
    "            twitter_graph.add_edge(str(follower), str(user))\n",
    "        except Exception as e:\n",
    "            print user, follower\n",
    "            print e\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the 'ego' edges\n",
    "following = pickle.load(open(\"following1\", \"rb\"))\n",
    "for node in following:\n",
    "    twitter_graph.add_edge(str(node), \"54679731\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for v in twitter_graph.vs.select(name_eq=\"54679731\"):\n",
    "#     print v\n",
    "    \n",
    "# for e in twitter_graph.es.select(_source=v.index): print e\n",
    "# for e in twitter_graph.es.select(_target=533): print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igraph.Vertex(<igraph.Graph object at 0x7faa9404f528>,544,{'name': '54679731'})\n"
     ]
    }
   ],
   "source": [
    "# for v in twitter_graph.vs:\n",
    "#     if len(twitter_graph.es.select(_source=v.index)) == 0 and len(twitter_graph.es.select(_target=v.index)) == 0:\n",
    "#         print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(twitter_graph, open(\"twitter_graph\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<igraph.drawing.Plot at 0x7faa8979c850>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = twitter_graph.layout_drl()\n",
    "plt1 = plot(twitter_graph, 'graph.drl.png', layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<igraph.drawing.Plot at 0x7faa8979c250>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = twitter_graph.layout(\"graphopt\")\n",
    "plt2 = plot(twitter_graph, 'graph.graphopt.png', layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layout = twitter_graph.layout(\"lgl\")\n",
    "plt2 = plot(twitter_graph, 'graph.lgl.png', layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://lists.nongnu.org/archive/html/igraph-help/2012-11/msg00047.html\n",
    "twitter_graph2 = twitter_graph.copy()\n",
    "nodes = twitter_graph2.vs(_degree_lt=100)\n",
    "twitter_graph2.es.select(_within=nodes).delete()\n",
    "twitter_graph2.vs(_degree_lt=50).delete()\n",
    "layout = twitter_graph2.layout_drl()\n",
    "plt1 = plot(twitter_graph2, 'graph2.drl.png', layout = layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<igraph.drawing.Plot at 0x7faa75259a50>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = twitter_graph.community_walktrap()\n",
    "plot(wc, 'cluser.walktrap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<igraph.drawing.Plot at 0x7faa752d2e10>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen = twitter_graph.community_leading_eigenvector()\n",
    "plot(eigen, 'cluser.eigen.png', mark_groups=True, bbox=(1500,1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<igraph.drawing.Plot at 0x7faa75259a10>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(eigen, 'cluser.eigen.png', mark_groups=True, bbox=(1500,1500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bios of Twitter Users\n",
    "\n",
    "Now that we have the graph communities, lets crawl twitter for the bios on each user. I will not reproduce the code, as it is mostly a copy of the above twitter crawl, using a different API call. Check biocrawl.py in the repo. We'll load the data from pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bios = pickle.load(open(\"bios\", \"rb\"))\n",
    "\n",
    "from collections import defaultdict\n",
    "documents = defaultdict(str)\n",
    "for v_idx, cluster in zip(range(len(eigen.membership)), eigen.membership):\n",
    "    twitter_id = int(twitter_graph.vs[v_idx].attributes()['name'])\n",
    "    if twitter_id in bios:\n",
    "        documents[cluster] += \"\\n%s\" % bios[ twitter_id ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Analysis\n",
    "\n",
    "Lets do a bigram analysis on the documents that created from a combination from user bio's and their latest tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preProcess(text):\n",
    "    text = text.lower()\n",
    "    #print text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #print tokens\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english') and not w.isdigit() and w != \"http\" and w != \"https\"]\n",
    "    #print filtered_words\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def getBigrams(content, threshold=5):\n",
    "    tokens = nltk.wordpunct_tokenize(preProcess(content))\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(threshold)\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    return sorted([ (bigram, score) for (bigram, score) in scored ], key=lambda t: t[1], reverse=True)\n",
    "\n",
    "def displayBigrams(content, threshold=5):\n",
    "    bigrams = getBigrams(content, threshold)\n",
    "    for bigram in bigrams:\n",
    "        print \" \".join(list(bigram[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'human', u'rights'), 0.0015222806531967894),\n",
       " ((u'co', u'host'), 0.0011071132023249377),\n",
       " ((u'national', u'security'), 0.0011071132023249377),\n",
       " ((u'pgp', u'co'), 0.0011071132023249377),\n",
       " ((u'foreign', u'policy'), 0.0009687240520343206),\n",
       " ((u'independent', u'journalist'), 0.0009687240520343206),\n",
       " ((u'middle', u'east'), 0.0008303349017437033),\n",
       " ((u'views', u'mine'), 0.0008303349017437033),\n",
       " ((u'civil', u'liberties'), 0.0006919457514530861),\n",
       " ((u'gmail', u'com'), 0.0006919457514530861),\n",
       " ((u'senior', u'editor'), 0.0006919457514530861),\n",
       " ((u'author', u'new'), 0.0005535566011624688),\n",
       " ((u'blog', u'co'), 0.0005535566011624688),\n",
       " ((u'co', u'founder'), 0.0005535566011624688),\n",
       " ((u'com', u'rt'), 0.0005535566011624688),\n",
       " ((u'new', u'book'), 0.0005535566011624688),\n",
       " ((u'rts', u'endorsements'), 0.0005535566011624688),\n",
       " ((u'al', u'jazeera'), 0.00041516745087185166),\n",
       " ((u'award', u'winning'), 0.00041516745087185166),\n",
       " ((u'center', u'economic'), 0.00041516745087185166),\n",
       " ((u'columnist', u'author'), 0.00041516745087185166),\n",
       " ((u'director', u'center'), 0.00041516745087185166),\n",
       " ((u'donald', u'trump'), 0.00041516745087185166),\n",
       " ((u'economic', u'policy'), 0.00041516745087185166),\n",
       " ((u'editor', u'chief'), 0.00041516745087185166),\n",
       " ((u'editor', u'co'), 0.00041516745087185166),\n",
       " ((u'endorsements', u'rt'), 0.00041516745087185166),\n",
       " ((u'founder', u'co'), 0.00041516745087185166),\n",
       " ((u'guardian', u'columnist'), 0.00041516745087185166),\n",
       " ((u'help', u'co'), 0.00041516745087185166),\n",
       " ((u'investigative', u'journalism'), 0.00041516745087185166),\n",
       " ((u'iran', u'deal'), 0.00041516745087185166),\n",
       " ((u'journalist', u'the_intercept'), 0.00041516745087185166),\n",
       " ((u'managing', u'editor'), 0.00041516745087185166),\n",
       " ((u'new', u'york'), 0.00041516745087185166),\n",
       " ((u'policy', u'research'), 0.00041516745087185166),\n",
       " ((u'rt', u'occupywallstnyc'), 0.00041516745087185166),\n",
       " ((u'sandra', u'bland'), 0.00041516745087185166),\n",
       " ((u'site', u'co'), 0.00041516745087185166),\n",
       " ((u'social', u'justice'), 0.00041516745087185166),\n",
       " ((u'treason', u'charges'), 0.00041516745087185166),\n",
       " ((u'writer', u'editor'), 0.00041516745087185166),\n",
       " ((u'account', u'tweets'), 0.0002767783005812344),\n",
       " ((u'activist', u'photographer'), 0.0002767783005812344),\n",
       " ((u'activist', u'recent'), 0.0002767783005812344),\n",
       " ((u'activist', u'writer'), 0.0002767783005812344),\n",
       " ((u'agenda', u'report'), 0.0002767783005812344),\n",
       " ((u'also', u'run'), 0.0002767783005812344),\n",
       " ((u'american', u'politics'), 0.0002767783005812344),\n",
       " ((u'americans', u'knew'), 0.0002767783005812344),\n",
       " ((u'analysis', u'current'), 0.0002767783005812344),\n",
       " ((u'angeles', u'times'), 0.0002767783005812344),\n",
       " ((u'apology', u'massincarceration'), 0.0002767783005812344),\n",
       " ((u'around', u'world'), 0.0002767783005812344),\n",
       " ((u'associate', u'editor'), 0.0002767783005812344),\n",
       " ((u'attack', u'free'), 0.0002767783005812344),\n",
       " ((u'barrett', u'brown'), 0.0002767783005812344),\n",
       " ((u'based', u'institute'), 0.0002767783005812344),\n",
       " ((u'bear', u'expect'), 0.0002767783005812344),\n",
       " ((u'bernie', u'sanders'), 0.0002767783005812344),\n",
       " ((u'bianca', u'jagger'), 0.0002767783005812344),\n",
       " ((u'bill', u'clinton'), 0.0002767783005812344),\n",
       " ((u'black', u'agenda'), 0.0002767783005812344),\n",
       " ((u'center', u'constitutional'), 0.0002767783005812344),\n",
       " ((u'class', u'co'), 0.0002767783005812344),\n",
       " ((u'clemencies', u'bill'), 0.0002767783005812344),\n",
       " ((u'climate', u'change'), 0.0002767783005812344),\n",
       " ((u'clinton', u'half'), 0.0002767783005812344),\n",
       " ((u'co', u'2udnjxwqjr'), 0.0002767783005812344),\n",
       " ((u'co', u'director'), 0.0002767783005812344),\n",
       " ((u'co', u'editor'), 0.0002767783005812344),\n",
       " ((u'co', u'journalist'), 0.0002767783005812344),\n",
       " ((u'co', u'ta2zig8ewu'), 0.0002767783005812344),\n",
       " ((u'cold', u'war'), 0.0002767783005812344),\n",
       " ((u'constitutional', u'rights'), 0.0002767783005812344),\n",
       " ((u'contact', u'co'), 0.0002767783005812344),\n",
       " ((u'contributing', u'editor'), 0.0002767783005812344),\n",
       " ((u'contributing', u'writer'), 0.0002767783005812344),\n",
       " ((u'cops', u'well'), 0.0002767783005812344),\n",
       " ((u'current', u'events'), 0.0002767783005812344),\n",
       " ((u'de', u'la'), 0.0002767783005812344),\n",
       " ((u'debunks', u'nuclear'), 0.0002767783005812344),\n",
       " ((u'deepa', u'kumar'), 0.0002767783005812344),\n",
       " ((u'deray', u'sandrabland'), 0.0002767783005812344),\n",
       " ((u'digital', u'rights'), 0.0002767783005812344),\n",
       " ((u'discourse', u'providing'), 0.0002767783005812344),\n",
       " ((u'documented', u'analysis'), 0.0002767783005812344),\n",
       " ((u'dot', u'com'), 0.0002767783005812344),\n",
       " ((u'dylann', u'roof'), 0.0002767783005812344),\n",
       " ((u'editor', u'author'), 0.0002767783005812344),\n",
       " ((u'editor', u'jacobin'), 0.0002767783005812344),\n",
       " ((u'email', u'co'), 0.0002767783005812344),\n",
       " ((u'email', u'pgp'), 0.0002767783005812344),\n",
       " ((u'enemy', u'within'), 0.0002767783005812344),\n",
       " ((u'etc', u'co'), 0.0002767783005812344),\n",
       " ((u'events', u'issues'), 0.0002767783005812344),\n",
       " ((u'ever', u'co'), 0.0002767783005812344),\n",
       " ((u'expect', u'make'), 0.0002767783005812344),\n",
       " ((u'follow', u'us'), 0.0002767783005812344),\n",
       " ((u'former', u'us'), 0.0002767783005812344),\n",
       " ((u'free', u'co'), 0.0002767783005812344),\n",
       " ((u'free', u'press'), 0.0002767783005812344),\n",
       " ((u'fundraising', u'campaign'), 0.0002767783005812344),\n",
       " ((u'gaza', u'co'), 0.0002767783005812344),\n",
       " ((u'global', u'affairs'), 0.0002767783005812344),\n",
       " ((u'great', u'work'), 0.0002767783005812344),\n",
       " ((u'hacking', u'team'), 0.0002767783005812344),\n",
       " ((u'half', u'apology'), 0.0002767783005812344),\n",
       " ((u'hillary', u'co'), 0.0002767783005812344),\n",
       " ((u'illegal', u'rude'), 0.0002767783005812344),\n",
       " ((u'independent', u'media'), 0.0002767783005812344),\n",
       " ((u'independent', u'research'), 0.0002767783005812344),\n",
       " ((u'institute', u'policy'), 0.0002767783005812344),\n",
       " ((u'intifada', u'co'), 0.0002767783005812344),\n",
       " ((u'investigative', u'journalist'), 0.0002767783005812344),\n",
       " ((u'investigative', u'reporter'), 0.0002767783005812344),\n",
       " ((u'iran', u'nuclear'), 0.0002767783005812344),\n",
       " ((u'iraq', u'war'), 0.0002767783005812344),\n",
       " ((u'iraqi', u'nuclear'), 0.0002767783005812344),\n",
       " ((u'ismashfizzle', u'illegal'), 0.0002767783005812344),\n",
       " ((u'israel', u'co'), 0.0002767783005812344),\n",
       " ((u'israel', u'palestine'), 0.0002767783005812344),\n",
       " ((u'ithaca', u'college'), 0.0002767783005812344),\n",
       " ((u'join', u'us'), 0.0002767783005812344),\n",
       " ((u'journalist', u'activist'), 0.0002767783005812344),\n",
       " ((u'journalist', u'author'), 0.0002767783005812344),\n",
       " ((u'journalist', u'democracy'), 0.0002767783005812344),\n",
       " ((u'journalist', u'writer'), 0.0002767783005812344),\n",
       " ((u'khorasan', u'group'), 0.0002767783005812344),\n",
       " ((u'kill', u'messenger'), 0.0002767783005812344),\n",
       " ((u'known', u'atheists'), 0.0002767783005812344),\n",
       " ((u'latin', u'american'), 0.0002767783005812344),\n",
       " ((u'lecturer', u'new'), 0.0002767783005812344),\n",
       " ((u'left', u'obama'), 0.0002767783005812344),\n",
       " ((u'left', u'right'), 0.0002767783005812344),\n",
       " ((u'left', u'wing'), 0.0002767783005812344),\n",
       " ((u'liberties', u'national'), 0.0002767783005812344),\n",
       " ((u'los', u'angeles'), 0.0002767783005812344),\n",
       " ((u'lt', u'lt'), 0.0002767783005812344),\n",
       " ((u'm', u'co'), 0.0002767783005812344),\n",
       " ((u'magazine', u'published'), 0.0002767783005812344),\n",
       " ((u'mainstream', u'media'), 0.0002767783005812344),\n",
       " ((u'many', u'people'), 0.0002767783005812344),\n",
       " ((u'massincarceration', u'hillary'), 0.0002767783005812344),\n",
       " ((u'maybe', u'co'), 0.0002767783005812344),\n",
       " ((u'media', u'critic'), 0.0002767783005812344),\n",
       " ((u'michael', u'cuesta'), 0.0002767783005812344),\n",
       " ((u'might', u'right'), 0.0002767783005812344),\n",
       " ((u'mine', u'rt'), 0.0002767783005812344),\n",
       " ((u'mine', u'rts'), 0.0002767783005812344),\n",
       " ((u'minimum', u'wage'), 0.0002767783005812344),\n",
       " ((u'much', u'like'), 0.0002767783005812344),\n",
       " ((u'myths', u'co'), 0.0002767783005812344),\n",
       " ((u'news', u'analysis'), 0.0002767783005812344),\n",
       " ((u'news', u'releases'), 0.0002767783005812344),\n",
       " ((u'news', u'site'), 0.0002767783005812344),\n",
       " ((u'newsletter', u'co'), 0.0002767783005812344),\n",
       " ((u'nuclear', u'myths'), 0.0002767783005812344),\n",
       " ((u'nuclear', u'scientist'), 0.0002767783005812344),\n",
       " ((u'ny', u'times'), 0.0002767783005812344),\n",
       " ((u'obama', u'token'), 0.0002767783005812344),\n",
       " ((u'officials', u'co'), 0.0002767783005812344),\n",
       " ((u'one', u'best'), 0.0002767783005812344),\n",
       " ((u'open', u'letter'), 0.0002767783005812344),\n",
       " ((u'opinions', u'mine'), 0.0002767783005812344),\n",
       " ((u'pentagon', u'papers'), 0.0002767783005812344),\n",
       " ((u'phd', u'student'), 0.0002767783005812344),\n",
       " ((u'poke', u'bear'), 0.0002767783005812344),\n",
       " ((u'policy', u'studies'), 0.0002767783005812344),\n",
       " ((u'political', u'analysis'), 0.0002767783005812344),\n",
       " ((u'politics', u'economics'), 0.0002767783005812344),\n",
       " ((u'press', u'co'), 0.0002767783005812344),\n",
       " ((u'project', u'censored'), 0.0002767783005812344),\n",
       " ((u'public', u'discourse'), 0.0002767783005812344),\n",
       " ((u'published', u'since'), 0.0002767783005812344),\n",
       " ((u'radio', u'host'), 0.0002767783005812344),\n",
       " ((u'radio', u'rt'), 0.0002767783005812344),\n",
       " ((u'radio', u'show'), 0.0002767783005812344),\n",
       " ((u'reporter', u'vicenews'), 0.0002767783005812344),\n",
       " ((u'research', u'ceprdc'), 0.0002767783005812344),\n",
       " ((u'review', u'co'), 0.0002767783005812344),\n",
       " ((u'rights', u'attorney'), 0.0002767783005812344),\n",
       " ((u'roots_action', u'co'), 0.0002767783005812344),\n",
       " ((u'rt', u'deray'), 0.0002767783005812344),\n",
       " ((u'rt', u'endorsement'), 0.0002767783005812344),\n",
       " ((u'rt', u'ismashfizzle'), 0.0002767783005812344),\n",
       " ((u'rt', u'the_intercept'), 0.0002767783005812344),\n",
       " ((u'rt', u'yashalevine'), 0.0002767783005812344),\n",
       " ((u'rude', u'cops'), 0.0002767783005812344),\n",
       " ((u'san', u'francisco'), 0.0002767783005812344),\n",
       " ((u'says', u're'), 0.0002767783005812344),\n",
       " ((u'scientist', u'debunks'), 0.0002767783005812344),\n",
       " ((u'social', u'scientist'), 0.0002767783005812344),\n",
       " ((u'socialist', u'web'), 0.0002767783005812344),\n",
       " ((u'specializing', u'economics'), 0.0002767783005812344),\n",
       " ((u'spring', u'spring'), 0.0002767783005812344),\n",
       " ((u'staff', u'reporter'), 0.0002767783005812344),\n",
       " ((u'state', u'dept'), 0.0002767783005812344),\n",
       " ((u'sweden', u'co'), 0.0002767783005812344),\n",
       " ((u'telling', u'truth'), 0.0002767783005812344),\n",
       " ((u'thanks', u'much'), 0.0002767783005812344),\n",
       " ((u'token', u'clemencies'), 0.0002767783005812344),\n",
       " ((u'tweets', u'mine'), 0.0002767783005812344),\n",
       " ((u'tweets', u'rt'), 0.0002767783005812344),\n",
       " ((u'u', u'foreign'), 0.0002767783005812344),\n",
       " ((u'uncivil', u'rites'), 0.0002767783005812344),\n",
       " ((u'us', u'co'), 0.0002767783005812344),\n",
       " ((u'us', u'restored'), 0.0002767783005812344),\n",
       " ((u'via', u'972mag'), 0.0002767783005812344),\n",
       " ((u'views', u'rt'), 0.0002767783005812344),\n",
       " ((u'village', u'voice'), 0.0002767783005812344),\n",
       " ((u'war', u'co'), 0.0002767783005812344),\n",
       " ((u'wars', u'co'), 0.0002767783005812344),\n",
       " ((u'web', u'site'), 0.0002767783005812344),\n",
       " ((u'well', u'documented'), 0.0002767783005812344),\n",
       " ((u'well', u'known'), 0.0002767783005812344),\n",
       " ((u'well', u'poke'), 0.0002767783005812344),\n",
       " ((u'white', u'house'), 0.0002767783005812344),\n",
       " ((u'world', u'socialist'), 0.0002767783005812344),\n",
       " ((u'writer', u'activist'), 0.0002767783005812344),\n",
       " ((u'writer', u'co'), 0.0002767783005812344),\n",
       " ((u'writer', u'musician'), 0.0002767783005812344),\n",
       " ((u'writer', u'reader'), 0.0002767783005812344),\n",
       " ((u'writer', u'researcher'), 0.0002767783005812344),\n",
       " ((u'york', u'times'), 0.0002767783005812344)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBigrams(documents[3], threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"We provide technology and expertise that help advance the global philanthropic community. We're here to help good takeover the world.\\nThe #npGameChangers are headed to ATL on 7/30 to share the latest tips, tricks, and tools in online fundraising http://t.co/xx0wEEwQRn\""
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[ twitter_id ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
